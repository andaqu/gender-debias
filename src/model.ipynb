{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bias detection using distributional models using the Maltese language\n",
    "\n",
    "## 0. Table of Contents\n",
    "\n",
    "**1. Pre-requisites:** must be run every time  \n",
    "**2. Load Corpus:** only if training  \n",
    "**3. Train models:** only if training  \n",
    "**4. Test models:** for testing saved models \n",
    "\n",
    "## 1. Pre-requisites\n",
    "\n",
    "Import all required libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from debiaswe.we import WordEmbedding\n",
    "from utils.weat import calculate_bias\n",
    "from gensim.models import TfidfModel\n",
    "from utils.callback import callback\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn.feature_extraction\n",
    "from utils.plot import plotBias\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from debiaswe import debias\n",
    "from gensim import corpora\n",
    "import sklearn.naive_bayes\n",
    "import gensim.downloader\n",
    "from debiaswe import we\n",
    "import sklearn.datasets\n",
    "import sklearn.manifold\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import pickle\n",
    "import scipy\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import re\n"
   ]
  },
  {
   "source": [
    "## 2. Load Corpus\n",
    "\n",
    "`load_traditional` $\\rightarrow$ load corpus document by document  \n",
    "`load_modern` $\\rightarrow$ load corpus token by token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "documents = []\n",
    "\n",
    "load_traditional = False\n",
    "load_modern = True\n",
    "\n",
    "for file in os.listdir(\"../dataset/dataset-neutral\"):\n",
    "\n",
    "    with open(f\"../dataset/dataset-neutral/{file}\", \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if load_traditional:\n",
    "        document = re.sub('[^a-zA-Z ]+', \"\", text)\n",
    "        documents.append(document) \n",
    "    elif load_modern:\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "            tokens = [token for token in tokens if re.match('^[a-zA-Z]+$', token)]\n",
    "            data.append(tokens)\n",
    "\n",
    "    print(f\"Finished with {file}!\")\n"
   ]
  },
  {
   "source": [
    "## 3. Train models\n",
    "\n",
    "### 3.1. Training Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    data,\n",
    "    size=100,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "word2vec_model.train(data, total_examples=len(data), epochs=10, compute_loss=True, callbacks=[callback()])\n",
    "\n",
    "# Save the model and embedding for later use.\n",
    "word2vec_model.save('../saved/neutral/word2vec/word2vec.model')\n",
    "word2vec_model.wv.save_word2vec_format('../saved/neutral/word2vec/word2vec_embedding.bin', binary=True)"
   ]
  },
  {
   "source": [
    "### 3.2. Training FastText"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText(\n",
    "    data, \n",
    "    size=100, \n",
    "    window=10, \n",
    "    min_count=2, \n",
    "    workers=10\n",
    ")\n",
    "\n",
    "fasttext_model.train(data, total_examples=len(data), epochs=10)\n",
    "\n",
    "fasttext_model.save('../saved/neutral/fasttext/fasttext.model')"
   ]
  },
  {
   "source": [
    "### 3.3. Fitting TF-IDF Vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_featuriser = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None)\n",
    "tfidf_featuriser.fit(documents)\n",
    "tfidf_docterm_matrix = tfidf_featuriser.transform(documents)\n",
    "\n",
    "pickle.dump(tfidf_featuriser, open(\"../saved/tfidf_vectorizer/tfidf_featuriser.pickle\", \"wb\"))\n",
    "pickle.dump(tfidf_docterm_matrix, open(\"../saved/tfidf_vectorizer/tfidf_docterm_matrix.pickle\", \"wb\"))"
   ]
  },
  {
   "source": [
    "## 4. Test models\n",
    "### 4.1. Test word2vec or FastText"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"original\"\n",
    "name = \"word2vec\"\n",
    "\n",
    "E = WordEmbedding(f\"../saved/{mode}/{name}/{name}.model\")"
   ]
  },
  {
   "source": [
    "Get gender direction $\\overrightarrow{g}$..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../saved/data/s0.json\", \"r\") as f:\n",
    "    s0 = json.load(f)\n",
    "g = we.doPCA(s0, E).components_[0]"
   ]
  },
  {
   "source": [
    "Get pairs in $Male:Female$ format ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"s1\"\n",
    "\n",
    "with open(f\"../saved/data/{name}.json\", \"r\") as f:\n",
    "    key_words = json.load(f)\n",
    "    print(\"Loaded maltese pairs...\")\n",
    "\n",
    "male = [p[0] for p in key_words]\n",
    "female = [p[1] for p in key_words]"
   ]
  },
  {
   "source": [
    "$O_1$: Calculate and visualise $|b_i|$ for every profession..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = {}\n",
    "fb = {}\n",
    "b = []\n",
    "\n",
    "for i, w in enumerate(key_words):\n",
    "    male = w[0]\n",
    "    female = w[1]\n",
    "\n",
    "    try:\n",
    "        mb[male] = E.v(male).dot(g)\n",
    "        fb[female] = E.v(female).dot(g)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    total_bias = mb[male] + fb[female]\n",
    "    b.append([female, male, total_bias])\n",
    "\n",
    "b = sorted(b, key=lambda x: x[2])\n",
    "extremes = b[0:20] + b[-20:]\n",
    "\n",
    "plotBias(mb, fb, extremes, title=\"Occupational Bias\")"
   ]
  },
  {
   "source": [
    "$O_2$: Use the $WEAT$ metric to quantify bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [x[0] for x in s0]\n",
    "F = [x[1] for x in s0]\n",
    "bias = calculate_bias(E, M, F, key_words)\n",
    "\n",
    "avg_bias = np.array([bias[k] for k in bias]).mean()\n",
    "\n",
    "print(f\"Average bias: {avg_bias}\")\n",
    "print(\"Top 20 biased terms according to the WEAT metric...\")\n",
    "dict(sorted(bias.items(), key=lambda x: x[1], reverse=True)[0:20])\n"
   ]
  },
  {
   "source": [
    "Hard debias word embedding given gender direction $\\overrightarrow{g}$, $E$ (equality sets) and $G$ (gender-specific words)..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../saved/data/equality_sets.json', \"r\") as f:\n",
    "    e = json.load(f)\n",
    "\n",
    "with open('../saved/data/gender_specific.json', \"r\") as f:\n",
    "    G = json.load(f)\n",
    "\n",
    "debias.debias(E, G, g, e, we)"
   ]
  },
  {
   "source": [
    "Generate best analogies..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gender = E.best_analogies_dist_thresh(g, topn=500, max_words=10000)\n",
    "\n",
    "for (a,b,c) in a_gender:\n",
    "    print(a+\"-\"+b)"
   ]
  },
  {
   "source": [
    "... or manual analogies $A : X :: B : Y$..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"ragel\"\n",
    "X = \"missier\"\n",
    "B = \"mara\"\n",
    "Y = E.most_similar(A, X, B, topn=10)\n",
    "\n",
    "print(f\"{A} : {X} :: {B} : ?\")\n",
    "print(f\"\\n\\nAnswers: {Y}\")"
   ]
  },
  {
   "source": [
    "### 4.2. Test TFIDF Vectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docterm_matrix = pickle.load(open(\"../saved/tfidf_vectorizer/tfidf_docterm_matrix.pickle\", \"rb\"))\n",
    "tfidf_featuriser = pickle.load(open(\"../saved/tfidf_vectorizer/tfidf_featuriser.pickle\", \"rb\"))\n",
    "\n",
    "print(tfidf_docterm_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"qasis\"\n",
    "X = \"pedofelu\"\n",
    "B = \"soru\"\n",
    "Y = \"omm\"\n",
    "\n",
    "vec1 = tfidf_docterm_matrix.transpose()[tfidf_featuriser.vocabulary_[A], :]\n",
    "vec2 = tfidf_docterm_matrix.transpose()[tfidf_featuriser.vocabulary_[X], :]\n",
    "vec3 = tfidf_docterm_matrix.transpose()[tfidf_featuriser.vocabulary_[B], :]\n",
    "vec4_truth = tfidf_docterm_matrix.transpose()[tfidf_featuriser.vocabulary_[Y], :]\n",
    "\n",
    "vec4_pred = vec2 + vec3 - vec1\n",
    "\n",
    "print(sklearn.metrics.pairwise.cosine_similarity(vec4_truth, vec4_pred)[0, 0])"
   ]
  }
 ]
}